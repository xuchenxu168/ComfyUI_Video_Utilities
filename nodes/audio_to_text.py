"""
Audio To Text Node
è¯­éŸ³è¯†åˆ«èŠ‚ç‚¹
"""

import os
import sys
import tempfile
import subprocess
import time
import random

from ..utils.asr_engine import ASREngine


def _log_info(message):
    print(f"[Audio_To_Text] {message}")


def _log_error(message):
    print(f"[Audio_To_Text ERROR] {message}")


def extract_video_path(video):
    """æå–è§†é¢‘è·¯å¾„ï¼ˆå…¼å®¹å¤šç§ VIDEO æ ¼å¼ï¼‰"""
    if isinstance(video, str):
        return video
    elif isinstance(video, dict):
        if 'filename' in video:
            import folder_paths
            video_dir = folder_paths.get_input_directory()
            return os.path.join(video_dir, video['filename'])
        elif 'saved_path' in video:
            return video['saved_path']
    elif hasattr(video, 'saved_path'):
        return video.saved_path
    return None


class VideoUtilitiesAudioToText:
    """è¯­éŸ³è¯†åˆ«èŠ‚ç‚¹"""

    # æ¨¡å‹åˆ—è¡¨
    MODELS_LIST = [
        # CTranslate2 æ ¼å¼ï¼ˆæ¨èï¼Œæ—  torchcodec ä¾èµ–ï¼‰
        "Belle-whisper-large-v3-zh-punct-ct2",  # æ¨èï¼šä¸­æ–‡ä¼˜åŒ–ï¼Œå¸¦æ ‡ç‚¹
        "Belle-whisper-large-v3-zh-punct-ct2-float32",  # é«˜ç²¾åº¦ä¸­æ–‡
        "whisper-large-v3-ct2",  # å¤šè¯­è¨€æ”¯æŒ

        # Transformers æ ¼å¼ï¼ˆéœ€è¦ torchcodecï¼Œå¯èƒ½åœ¨ Windows ä¸Šæœ‰é—®é¢˜ï¼‰
        "Belle-whisper-large-v3-zh-punct",  # ä¸­æ–‡ä¼˜åŒ–ï¼Œå¸¦æ ‡ç‚¹ï¼ˆTransformersï¼‰
        "Belle-whisper-large-v3-zh",  # ä¸­æ–‡ä¼˜åŒ–ï¼ˆTransformersï¼‰
    ]

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": (cls.MODELS_LIST, {"default": cls.MODELS_LIST[0], "tooltip": "é€‰æ‹©ASRæ¨¡å‹ï¼Œä¸­æ–‡æ¨èä½¿ç”¨ zh æ¨¡å‹"}),
                "max_sentence_length": ("INT", {"default": 20, "min": 1, "max": 1000, "step": 1, "tooltip": "ä¸­æ–‡æŒ‰å­—æ•°è®¡ç®—ï¼Œè‹±æ–‡æŒ‰å­—æ¯æ•°è®¡ç®—"}),
                "unload_model": ("BOOLEAN", {"default": True, "tooltip": "è¿è¡Œåå¸è½½æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xFFFFFFFFFFFFFFFF, "step": 1, "tooltip": "éšæœºç§å­"}),
            },
            "optional": {
                "video": ("VIDEO", {"tooltip": "å¯é€‰ï¼šè¾“å…¥è§†é¢‘æ–‡ä»¶"}),
                "audio": ("AUDIO", {"tooltip": "å¯é€‰ï¼šè¾“å…¥éŸ³é¢‘æ–‡ä»¶"}),
                "prompt": ("STRING", {"multiline": False, "default": "", "placeholder": "å¯é€‰ï¼šè¾“å…¥æç¤ºæ–‡æœ¬æ¥å¼•å¯¼è½¬å½•ï¼ˆä¾‹å¦‚ï¼šä¸“ä¸šæœ¯è¯­ã€äººåã€åœ°åç­‰ï¼‰"}),
                "reference_text": ("STRING", {"multiline": True, "default": "", "placeholder": "å¯é€‰ï¼šè¾“å…¥å‡†ç¡®çš„å‚è€ƒæ–‡æœ¬ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ ¡æ­£è½¬å½•ç»“æœï¼ˆæ¯è¡Œä¸€å¥ï¼‰"}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("çº¯æ–‡æœ¬", "é€è¯æ—¶é—´æˆ³", "é€å¥æ—¶é—´æˆ³")
    FUNCTION = "recognize"
    CATEGORY = "Video_Utilities/ASR"

    def __init__(self):
        self.asr_engine = ASREngine()
    
    def recognize(
        self,
        model,
        max_sentence_length,
        unload_model,
        seed,
        video=None,
        audio=None,
        prompt="",
        reference_text=""
    ):
        """
        è¯­éŸ³è¯†åˆ«ï¼ˆæ”¯æŒè§†é¢‘æˆ–éŸ³é¢‘è¾“å…¥ï¼‰

        Returns:
            (çº¯æ–‡æœ¬, é€è¯æ—¶é—´æˆ³, é€å¥æ—¶é—´æˆ³)
        """
        try:
            # è®¾ç½®éšæœºç§å­
            if seed != 0:
                import torch
                torch.manual_seed(seed)
                torch.cuda.manual_seed_all(seed)

            # 1. æ£€æŸ¥è¾“å…¥
            if video is None and audio is None:
                _log_error("âŒ å¿…é¡»æä¾›è§†é¢‘æˆ–éŸ³é¢‘è¾“å…¥")
                return ("", "", "")

            if video is not None and audio is not None:
                _log_error("âŒ ä¸èƒ½åŒæ—¶æä¾›è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥ï¼Œè¯·åªé€‰æ‹©ä¸€ä¸ª")
                return ("", "", "")

            # 2. è·å–éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            audio_path = None
            need_cleanup_audio = False  # æ ‡è®°æ˜¯å¦éœ€è¦æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶

            if video is not None:
                # ä»è§†é¢‘ä¸­æå–éŸ³é¢‘
                video_path = extract_video_path(video)
                if not video_path or not os.path.exists(video_path):
                    _log_error("âŒ æ— æ³•è·å–è§†é¢‘è·¯å¾„æˆ–è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
                    return ("", "", "")

                _log_info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
                _log_info("ğŸµ æ­£åœ¨æå–éŸ³é¢‘...")
                audio_path = self._extract_audio(video_path)
                if not audio_path:
                    _log_error("âŒ éŸ³é¢‘æå–å¤±è´¥")
                    return ("", "", "")

                need_cleanup_audio = True  # ä»è§†é¢‘æå–çš„éŸ³é¢‘éœ€è¦æ¸…ç†

            elif audio is not None:
                # å¤„ç†éŸ³é¢‘è¾“å…¥
                # AUDIO ç±»å‹æ˜¯ä¸€ä¸ªå­—å…¸: {"waveform": tensor, "sample_rate": int}
                # éœ€è¦å°†å…¶ä¿å­˜ä¸ºä¸´æ—¶ WAV æ–‡ä»¶
                if isinstance(audio, dict) and "waveform" in audio and "sample_rate" in audio:
                    _log_info("ğŸµ æ£€æµ‹åˆ°éŸ³é¢‘å¼ é‡ï¼Œæ­£åœ¨ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶...")
                    import torchaudio

                    # ç”Ÿæˆä¸´æ—¶éŸ³é¢‘æ–‡ä»¶è·¯å¾„
                    audio_path = os.path.join(
                        tempfile.gettempdir(),
                        f"audio_{int(time.time())}_{random.randint(1000, 9999)}.wav"
                    )

                    # ä¿å­˜éŸ³é¢‘
                    waveform = audio["waveform"].squeeze(0)  # ç§»é™¤ batch ç»´åº¦
                    sample_rate = audio["sample_rate"]
                    torchaudio.save(audio_path, waveform, sample_rate)

                    _log_info(f"ğŸµ éŸ³é¢‘å·²ä¿å­˜åˆ°: {audio_path}")

                    # æ ‡è®°éœ€è¦æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    need_cleanup_audio = True
                else:
                    # å°è¯•ä½œä¸ºæ–‡ä»¶è·¯å¾„å¤„ç†
                    audio_path = extract_video_path(audio)
                    if not audio_path or not os.path.exists(audio_path):
                        _log_error(f"âŒ æ— æ³•è·å–éŸ³é¢‘è·¯å¾„æˆ–éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œaudio ç±»å‹: {type(audio)}")
                        return ("", "", "")

                    _log_info(f"ğŸµ å¼€å§‹å¤„ç†éŸ³é¢‘: {audio_path}")
                    need_cleanup_audio = False

            # 3. è¯­éŸ³è¯†åˆ«ï¼ˆä½¿ç”¨æ–°çš„æ¨¡å‹å‚æ•°ï¼‰
            _log_info(f"ğŸ¤ æ­£åœ¨ä½¿ç”¨æ¨¡å‹ {model} è¯†åˆ«éŸ³é¢‘...")
            words_list, sentences_list = self.asr_engine.recognize(
                audio_path,
                "faster-whisper",  # å›ºå®šä½¿ç”¨ faster-whisper å¼•æ“
                model,  # ä¼ é€’æ¨¡å‹åç§°
                "auto",  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
                prompt,
                max_sentence_length
            )

            # æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if need_cleanup_audio:
                try:
                    os.unlink(audio_path)
                    _log_info(f"ğŸ—‘ï¸ å·²æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶: {audio_path}")
                except Exception as e:
                    _log_error(f"âš ï¸ æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
            
            # 4. åº”ç”¨å‚è€ƒæ–‡æœ¬æ ¡æ­£ï¼ˆå¦‚æœæä¾›ï¼‰
            if reference_text and reference_text.strip():
                _log_info("ğŸ”§ æ­£åœ¨ä½¿ç”¨å‚è€ƒæ–‡æœ¬æ ¡æ­£è¯†åˆ«ç»“æœ...")
                sentences_list = self._correct_with_reference(sentences_list, reference_text)
                # é‡æ–°ç”Ÿæˆé€è¯æ—¶é—´æˆ³ï¼ˆä½¿ç”¨ jieba åˆ†è¯ï¼‰
                _log_info("ğŸ”§ é‡æ–°ç”Ÿæˆé€è¯æ—¶é—´æˆ³...")
                words_list = self.asr_engine._generate_words_from_sentences(sentences_list)

            # 5. æ ¼å¼åŒ–è¾“å‡º
            plain_text = self._format_plain_text(sentences_list)
            words_timestamps = self._format_words_timestamps(words_list)
            sentences_timestamps = self._format_sentences_timestamps(sentences_list)
            
            _log_info(f"âœ… è¯†åˆ«å®Œæˆ: {len(sentences_list)} ä¸ªå¥å­, {len(words_list)} ä¸ªè¯")

            # 6. å¸è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if unload_model:
                self.asr_engine.unload_model()

            return (plain_text, words_timestamps, sentences_timestamps)
        
        except Exception as e:
            _log_error(f"âŒ è¯†åˆ«å¤±è´¥: {str(e)}")
            import traceback
            _log_error(traceback.format_exc())
            return ("", "", "")
    
    def _extract_audio(self, video_path):
        """æå–è§†é¢‘éŸ³é¢‘ä¸º WAV æ ¼å¼"""
        try:
            # ç”Ÿæˆä¸´æ—¶éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            audio_path = os.path.join(
                tempfile.gettempdir(),
                f"audio_{int(time.time())}_{random.randint(1000, 9999)}.wav"
            )
            
            cmd = [
                "ffmpeg", "-i", video_path,
                "-vn",  # ä¸å¤„ç†è§†é¢‘
                "-acodec", "pcm_s16le",  # PCM 16-bit
                "-ar", "16000",  # 16kHz é‡‡æ ·ç‡ï¼ˆWhisper æ¨èï¼‰
                "-ac", "1",  # å•å£°é“
                "-y",  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                audio_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0 and os.path.exists(audio_path):
                _log_info(f"âœ… éŸ³é¢‘æå–æˆåŠŸ: {audio_path}")
                return audio_path
            else:
                _log_error(f"âŒ FFmpeg é”™è¯¯: {result.stderr}")
                return None
        
        except Exception as e:
            _log_error(f"âŒ éŸ³é¢‘æå–å¤±è´¥: {str(e)}")
            return None
    
    def _correct_with_reference(self, sentences_list, reference_text):
        """
        ä½¿ç”¨å‚è€ƒæ–‡æœ¬æ ¡æ­£è¯†åˆ«ç»“æœ

        **å®Œå…¨ä»¥å‚è€ƒæ–‡æœ¬ä¸ºå‡†**ï¼š
        - å‚è€ƒæ–‡æœ¬æœ‰å¤šå°‘è¡Œï¼Œå°±ç”Ÿæˆå¤šå°‘ä¸ªå¥å­
        - æ—¶é—´æˆ³æ ¹æ®è¯†åˆ«ç»“æœçš„æ€»æ—¶é•¿æŒ‰å­—ç¬¦æ•°æ¯”ä¾‹åˆ†é…

        Args:
            sentences_list: åŸå§‹è¯†åˆ«ç»“æœ [(start, end, text), ...]
            reference_text: å‚è€ƒæ–‡æœ¬ï¼ˆå¤šè¡Œï¼Œæ¯è¡Œä¸€ä¸ªå¥å­ï¼‰
                          æ”¯æŒå»¶é•¿æ˜¾ç¤ºæ—¶é•¿ï¼šåœ¨è¡Œæœ«æ·»åŠ  -æ•°å­—sï¼Œä¾‹å¦‚ "è¿™æ˜¯ä¸€å¥è¯ -0.5s" è¡¨ç¤ºå»¶é•¿0.5ç§’

        Returns:
            corrected_sentences_list: æ ¡æ­£åçš„å¥å­åˆ—è¡¨
        """
        import re

        # 1. è§£æå‚è€ƒæ–‡æœ¬ï¼ˆæŒ‰è¡Œåˆ†å‰²ï¼Œå¹¶æå–å»¶é•¿æ—¶é•¿ï¼‰
        reference_lines = []
        extend_durations = {}  # {è¡Œç´¢å¼•: å»¶é•¿æ—¶é•¿}

        for idx, line in enumerate(reference_text.strip().split('\n')):
            line = line.strip()
            if not line:
                continue

            # æ£€æŸ¥æ˜¯å¦æœ‰å»¶é•¿æ—¶é•¿æ ‡è®°ï¼ˆæ ¼å¼ï¼š-æ•°å­—sï¼‰
            duration_match = re.search(r'[\sï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€]*-(\d+(?:\.\d+)?)s\s*$', line)
            if duration_match:
                extend_duration = float(duration_match.group(1))
                # ç§»é™¤æ—¶é•¿æ ‡è®°ï¼Œä¿ç•™æ–‡æœ¬
                line_text = line[:duration_match.start()].strip()
                extend_durations[len(reference_lines)] = extend_duration
                _log_info(f"ğŸ“ æ£€æµ‹åˆ°å»¶é•¿æ—¶é•¿: ç¬¬ {len(reference_lines) + 1} å¥å»¶é•¿ {extend_duration} ç§’")
            else:
                line_text = line

            if line_text:  # ç¡®ä¿ä¸ä¸ºç©º
                reference_lines.append(line_text)

        if not reference_lines:
            _log_info("âš ï¸ å‚è€ƒæ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡æ ¡æ­£")
            return sentences_list

        # 2. è·å–è¯†åˆ«ç»“æœçš„æ€»æ—¶é•¿
        total_start = sentences_list[0][0]
        total_end = sentences_list[-1][1]
        total_duration = total_end - total_start

        _log_info(f"ğŸ“Š å‚è€ƒæ–‡æœ¬: {len(reference_lines)} è¡Œ")
        _log_info(f"ğŸ“Š è¯†åˆ«ç»“æœ: {len(sentences_list)} ä¸ªå¥å­")
        _log_info(f"ğŸ“Š æ€»æ—¶é•¿: {total_duration:.2f}s ({total_start:.2f}s - {total_end:.2f}s)")

        # 3. æŒ‰å­—ç¬¦æ•°æ¯”ä¾‹åˆ†é…æ—¶é—´æˆ³
        total_chars = sum(len(line) for line in reference_lines)
        aligned_sentences = []
        current_time = total_start

        for idx, line in enumerate(reference_lines):
            line_start = current_time

            # å…ˆæŒ‰å­—ç¬¦æ•°è®¡ç®—åŸºç¡€æ—¶é•¿
            base_duration = (len(line) / total_chars) * total_duration

            # æ£€æŸ¥æ˜¯å¦æœ‰å»¶é•¿æ—¶é•¿
            if idx in extend_durations:
                extend_duration = extend_durations[idx]
                line_duration = base_duration + extend_duration
                _log_info(f"â±ï¸ ç¬¬ {idx+1} å¥å»¶é•¿ {extend_duration} ç§’ (åŸºç¡€: {base_duration:.2f}s â†’ å»¶é•¿å: {line_duration:.2f}s)")
            else:
                line_duration = base_duration

            line_end = current_time + line_duration
            aligned_sentences.append((round(line_start, 2), round(line_end, 2), line))
            current_time = line_end

        _log_info(f"âœ… å‚è€ƒæ–‡æœ¬æ ¡æ­£å®Œæˆ: {len(aligned_sentences)} ä¸ªå¥å­")
        return aligned_sentences


    def _format_plain_text(self, sentences_list):
        """æ ¼å¼åŒ–çº¯æ–‡æœ¬"""
        if not sentences_list:
            return ""
        return "\n".join([text for _, _, text in sentences_list])
    
    def _format_words_timestamps(self, words_list):
        """æ ¼å¼åŒ–é€è¯æ—¶é—´æˆ³"""
        if not words_list:
            return ""
        lines = []
        for start, end, word in words_list:
            lines.append(f"({start}, {end}) {word}")
        return "\n".join(lines)
    
    def _format_sentences_timestamps(self, sentences_list):
        """æ ¼å¼åŒ–é€å¥æ—¶é—´æˆ³"""
        if not sentences_list:
            return ""
        lines = []
        for start, end, text in sentences_list:
            lines.append(f"({start}, {end}) {text}")
        return "\n".join(lines)

