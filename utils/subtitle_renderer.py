"""
å­—å¹•æ¸²æŸ“å™¨
æ”¯æŒé™æ€å’ŒåŠ¨æ€å­—å¹•ï¼Œä¿ç•™æ‰€æœ‰ç°æœ‰åŠŸèƒ½
"""

import os
import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from typing import List, Tuple, Optional
from .animation import apply_animation
from .text_wrapper import smart_wrap_text, remove_punctuation


def _log_info(message):
    print(f"[SubtitleRenderer] {message}")


def _log_error(message):
    print(f"[SubtitleRenderer ERROR] {message}")


class SubtitleRenderer:
    """å­—å¹•æ¸²æŸ“å™¨"""
    
    def __init__(self, video_width: int, video_height: int, fps: float):
        self.video_width = video_width
        self.video_height = video_height
        self.fps = fps
    
    def render_static_subtitles(
        self,
        video_path: str,
        output_path: str,
        sentences_list: List[Tuple[float, float, str]],
        font_path: str,
        font_size: int,
        font_color: str,
        text_direction: str,
        position: str,
        background: str,
        animation: str,
        animation_duration: float,
        stroke_width: float = 2.0,
        stroke_color: str = "black",
        subtitle_extend_time: float = 0.0,
        offset_x: int = 0,
        offset_y: int = 0
    ):
        """
        æ¸²æŸ“é™æ€å­—å¹•

        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            sentences_list: å¥å­åˆ—è¡¨ [(start, end, text), ...]
            å…¶ä»–å‚æ•°: å­—å¹•æ ·å¼å‚æ•°
        """
        _log_info(f"ğŸ¬ å¼€å§‹æ¸²æŸ“é™æ€å­—å¹•ï¼Œå…± {len(sentences_list)} ä¸ªå¥å­")

        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")

        # è·å–è§†é¢‘ä¿¡æ¯
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # åŠ è½½å­—ä½“
        try:
            font = ImageFont.truetype(font_path, font_size)
        except Exception as e:
            _log_error(f"åŠ è½½å­—ä½“å¤±è´¥: {e}")
            font = ImageFont.load_default()

        # é¢œè‰²æ˜ å°„
        color_map = {
            "white": (255, 255, 255),
            "yellow": (255, 255, 0),
            "black": (0, 0, 0),
            "red": (255, 0, 0),
            "green": (0, 255, 0),
            "blue": (0, 0, 255)
        }
        font_rgb = color_map.get(font_color, (255, 255, 0))
        stroke_rgb = color_map.get(stroke_color, (0, 0, 0)) if stroke_width > 0 else None

        # èƒŒæ™¯è®¾ç½®
        bg_opacity = 0.5 if background == "yes" else 0.0
        bg_rgb = (0, 0, 0)  # é»‘è‰²èƒŒæ™¯
        
        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, self.fps, (self.video_width, self.video_height))
        
        # é€å¸§å¤„ç†
        frame_idx = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_time = frame_idx / self.fps
            
            # æŸ¥æ‰¾å½“å‰æ—¶é—´çš„å­—å¹•
            current_subtitle = None
            subtitle_start = 0
            subtitle_end = 0
            
            for start, end, text in sentences_list:
                # å»¶é•¿å­—å¹•æ˜¾ç¤ºæ—¶é—´
                extended_end = end + subtitle_extend_time
                if start <= current_time <= extended_end:
                    # å»é™¤æ ‡ç‚¹ç¬¦å·
                    import string
                    punctuation = string.punctuation + 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹ã€Â·â€¦â€”'
                    current_subtitle = ''.join([c for c in text if c not in punctuation])
                    subtitle_start = start
                    subtitle_end = extended_end
                    break

            # å¦‚æœæœ‰å­—å¹•ï¼Œç»˜åˆ¶
            if current_subtitle and current_subtitle.strip():
                # è®¡ç®—åŠ¨ç”»è¿›åº¦
                subtitle_duration = subtitle_end - subtitle_start
                subtitle_elapsed = current_time - subtitle_start
                subtitle_progress = subtitle_elapsed / subtitle_duration if subtitle_duration > 0 else 1.0

                # åŠ¨ç”»è¿›åº¦ï¼ˆå‰ animation_duration ç§’ï¼‰
                if subtitle_elapsed < animation_duration:
                    animation_progress = subtitle_elapsed / animation_duration
                else:
                    animation_progress = 1.0

                # ç»˜åˆ¶å­—å¹•
                frame = self._draw_subtitle_on_frame(
                    frame,
                    current_subtitle,
                    font,
                    font_rgb,
                    bg_rgb,
                    bg_opacity,
                    text_direction,
                    position,
                    stroke_width,
                    stroke_rgb,
                    animation,
                    animation_progress,
                    subtitle_progress,
                    offset_x,
                    offset_y
                )
            
            out.write(frame)
            frame_idx += 1
            
            # è¿›åº¦æ˜¾ç¤º
            if frame_idx % 30 == 0:
                progress = (frame_idx / total_frames) * 100
                _log_info(f"è¿›åº¦: {progress:.1f}% ({frame_idx}/{total_frames})")
        
        cap.release()
        out.release()

        # ä½¿ç”¨ ffmpeg åˆå¹¶éŸ³é¢‘
        _log_info(f"ğŸµ æ­£åœ¨åˆå¹¶éŸ³é¢‘...")
        self._merge_audio(video_path, output_path)

        _log_info(f"âœ… é™æ€å­—å¹•æ¸²æŸ“å®Œæˆ: {output_path}")
    
    def _draw_subtitle_on_frame(
        self,
        frame: np.ndarray,
        text: str,
        font: ImageFont.FreeTypeFont,
        font_color: Tuple[int, int, int],
        bg_color: Tuple[int, int, int],
        bg_opacity: float,
        text_direction: str,
        position: str,
        stroke_width: float,
        stroke_color: Optional[Tuple[int, int, int]],
        animation: str,
        animation_progress: float,
        subtitle_progress: float,
        offset_x: int = 0,
        offset_y: int = 0
    ) -> np.ndarray:
        """åœ¨å¸§ä¸Šç»˜åˆ¶å­—å¹•"""

        # è½¬æ¢ä¸º PIL Image (RGB)
        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_image, 'RGBA')

        # è®¡ç®—æ–‡å­—å°ºå¯¸å’Œä½ç½®
        wrapped_text = text  # é»˜è®¤å€¼
        if text_direction == "vertical":
            text_width, text_height = self._get_vertical_text_size(draw, text, font)
        else:
            # æ™ºèƒ½æ¢è¡Œ
            max_width = int(self.video_width * 0.9)
            wrapped_text = smart_wrap_text(text, max_width, font.path, font.size, 'zh')
            bbox = draw.multiline_textbbox((0, 0), wrapped_text, font=font, spacing=3)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]

        # è®¡ç®—åŸºç¡€ä½ç½®
        # ä½ç½®æ ¼å¼ï¼šå³ä¸Šã€å³ä¸­ã€å³ä¸‹ã€ä¸­ä¸Šã€æ­£ä¸­ã€ä¸­ä¸‹ã€å·¦ä¸Šã€å·¦ä¸­ã€å·¦ä¸‹
        # å…¼å®¹æ—§æ ¼å¼ï¼šbottom -> ä¸­ä¸‹, top -> ä¸­ä¸Š, middle -> æ­£ä¸­
        position_map = {
            "bottom": "ä¸­ä¸‹",
            "top": "ä¸­ä¸Š",
            "middle": "æ­£ä¸­",
            "ä¸­ä¸­": "æ­£ä¸­"  # å…¼å®¹æ—§çš„"ä¸­ä¸­"
        }
        position = position_map.get(position, position)

        # è§£æä½ç½®
        if position in ["å³ä¸Š", "å³ä¸­", "å³ä¸‹"]:
            # å³ä¾§
            horizontal_offset = int(self.video_width * 0.05)
            base_x = self.video_width - text_width - horizontal_offset
        elif position in ["ä¸­ä¸Š", "æ­£ä¸­", "ä¸­ä¸‹"]:
            # ä¸­é—´
            base_x = (self.video_width - text_width) // 2
        else:  # å·¦ä¸Šã€å·¦ä¸­ã€å·¦ä¸‹
            # å·¦ä¾§
            horizontal_offset = int(self.video_width * 0.05)
            base_x = horizontal_offset

        if position in ["å³ä¸Š", "ä¸­ä¸Š", "å·¦ä¸Š"]:
            # ä¸Šæ–¹
            vertical_offset = int(self.video_height * 0.05)
            base_y = vertical_offset
        elif position in ["å³ä¸­", "æ­£ä¸­", "å·¦ä¸­"]:
            # ä¸­é—´
            base_y = (self.video_height - text_height) // 2
        else:  # å³ä¸‹ã€ä¸­ä¸‹ã€å·¦ä¸‹
            # ä¸‹æ–¹
            vertical_offset = int(self.video_height * 0.05)
            base_y = self.video_height - text_height - vertical_offset

        # åº”ç”¨åç§»é‡
        # offset_x: æ­£æ•°å‘å³ï¼Œè´Ÿæ•°å‘å·¦
        # offset_y: æ­£æ•°å‘ä¸Šï¼Œè´Ÿæ•°å‘ä¸‹ï¼ˆæ³¨æ„ï¼šå›¾åƒåæ ‡ç³» Y è½´å‘ä¸‹ï¼Œæ‰€ä»¥è¦å–åï¼‰
        base_x += offset_x
        base_y -= offset_y

        # åº”ç”¨åŠ¨ç”»æ•ˆæœ
        x, y, alpha, scale, color = apply_animation(
            animation,
            animation_progress,
            subtitle_progress,
            base_x,
            base_y,
            font_color,
            self.video_width,
            self.video_height,
            text_height
        )

        # åº”ç”¨ç¼©æ”¾æ•ˆæœï¼ˆå¦‚æœéœ€è¦ï¼‰
        scaled_font = font
        scaled_text_width = text_width
        scaled_text_height = text_height

        if scale != 1.0 and scale > 0.1:  # ç¡®ä¿ç¼©æ”¾æ¯”ä¾‹ä¸ä¼šå¤ªå°
            # åˆ›å»ºç¼©æ”¾åçš„å­—ä½“
            scaled_font_size = max(1, int(font.size * scale))  # ç¡®ä¿å­—ä½“å¤§å°è‡³å°‘ä¸º1
            scaled_font = ImageFont.truetype(font.path, scaled_font_size)

            # é‡æ–°è®¡ç®—æ–‡å­—å°ºå¯¸
            if text_direction == "vertical":
                scaled_text_width, scaled_text_height = self._get_vertical_text_size(draw, text, scaled_font)
            else:
                bbox = draw.multiline_textbbox((0, 0), wrapped_text, font=scaled_font, spacing=3)
                scaled_text_width = bbox[2] - bbox[0]
                scaled_text_height = bbox[3] - bbox[1]

            # è°ƒæ•´ä½ç½®ä»¥ä¿æŒå±…ä¸­
            x = x + (text_width - scaled_text_width) // 2
            y = y + (text_height - scaled_text_height) // 2

        # ç»˜åˆ¶èƒŒæ™¯
        if bg_opacity > 0:
            bg_padding = 10
            bg_x1 = x - bg_padding
            bg_y1 = y - bg_padding
            bg_x2 = x + scaled_text_width + bg_padding
            bg_y2 = y + scaled_text_height + bg_padding
            draw.rectangle(
                [bg_x1, bg_y1, bg_x2, bg_y2],
                fill=(*bg_color, int(255 * bg_opacity * alpha))
            )

        # ç»˜åˆ¶æ–‡å­—
        if text_direction == "vertical":
            self._draw_vertical_text(draw, text, scaled_font, x, y, color, alpha, stroke_width, stroke_color)
        else:
            # æè¾¹æ•ˆæœ
            if stroke_width > 0 and stroke_color:
                for stroke_offset_x in range(-int(stroke_width), int(stroke_width) + 1):
                    for stroke_offset_y in range(-int(stroke_width), int(stroke_width) + 1):
                        if stroke_offset_x != 0 or stroke_offset_y != 0:
                            draw.multiline_text(
                                (x + stroke_offset_x, y + stroke_offset_y),
                                wrapped_text,
                                font=scaled_font,
                                fill=(*stroke_color, int(255 * alpha)),
                                spacing=3,
                                align="center"
                            )

            # ä¸»æ–‡å­—
            draw.multiline_text(
                (x, y),
                wrapped_text,
                font=scaled_font,
                fill=(*color, int(255 * alpha)),
                spacing=3,
                align="center"
            )
        
        # è½¬æ¢å› OpenCV æ ¼å¼ (BGR)
        return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
    
    def _draw_vertical_text(
        self,
        draw: ImageDraw.ImageDraw,
        text: str,
        font: ImageFont.FreeTypeFont,
        x: int,
        y: int,
        color: Tuple[int, int, int],
        alpha: float,
        stroke_width: float,
        stroke_color: Optional[Tuple[int, int, int]]
    ):
        """ç»˜åˆ¶ç«–æ’æ–‡å­—"""
        try:
            line_height = font.size
        except:
            bbox = draw.textbbox((0, 0), "æµ‹", font=font)
            line_height = bbox[3] - bbox[1]
        
        current_y = y
        for char in text:
            # æè¾¹
            if stroke_width > 0 and stroke_color:
                for stroke_offset_x in range(-int(stroke_width), int(stroke_width) + 1):
                    for stroke_offset_y in range(-int(stroke_width), int(stroke_width) + 1):
                        if stroke_offset_x != 0 or stroke_offset_y != 0:
                            draw.text(
                                (x + stroke_offset_x, current_y + stroke_offset_y),
                                char,
                                font=font,
                                fill=(*stroke_color, int(255 * alpha))
                            )

            # ä¸»æ–‡å­—
            draw.text((x, current_y), char, font=font, fill=(*color, int(255 * alpha)))
            current_y += line_height
    
    def _get_vertical_text_size(
        self,
        draw: ImageDraw.ImageDraw,
        text: str,
        font: ImageFont.FreeTypeFont
    ) -> Tuple[int, int]:
        """è®¡ç®—ç«–æ’æ–‡å­—å°ºå¯¸"""
        try:
            line_height = font.size
        except:
            bbox = draw.textbbox((0, 0), "æµ‹", font=font)
            line_height = bbox[3] - bbox[1]
        
        total_height = line_height * len(text)
        max_width = 0
        for char in text:
            bbox = draw.textbbox((0, 0), char, font=font)
            char_width = bbox[2] - bbox[0]
            max_width = max(max_width, char_width)
        
        return max_width, total_height
    
    def _hex_to_rgb(self, hex_color: str) -> Tuple[int, int, int]:
        """åå…­è¿›åˆ¶é¢œè‰²è½¬ RGB"""
        hex_color = hex_color.lstrip('#')
        if len(hex_color) == 6:
            return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))
        return (255, 255, 0)  # é»˜è®¤é»„è‰²

    def render_dynamic_subtitles(
        self,
        video_path: str,
        output_path: str,
        words_list: List[Tuple[float, float, str]],
        font_path: str,
        font_size: int,
        font_color: str,
        text_direction: str,
        position: str,
        background: str,
        max_lines: int = 3,
        clearance_threshold: float = 2.0,
        stroke_width: float = 3.0,
        stroke_color: str = "black",
        subtitle_extend_time: float = 0.0,
        offset_x: int = 0,
        offset_y: int = 0
    ):
        """
        æ¸²æŸ“åŠ¨æ€å­—å¹•ï¼ˆé€è¯ç´¯ç§¯æ˜¾ç¤ºï¼‰

        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            words_list: è¯åˆ—è¡¨ [(start, end, word), ...]
            å…¶ä»–å‚æ•°: å­—å¹•æ ·å¼å‚æ•°
        """
        _log_info(f"ğŸ¯ å¼€å§‹æ¸²æŸ“åŠ¨æ€å­—å¹•ï¼Œå…± {len(words_list)} ä¸ªè¯")

        if not words_list:
            _log_error("âŒ é€è¯æ—¶é—´æˆ³ä¸ºç©ºï¼Œæ— æ³•æ¸²æŸ“åŠ¨æ€å­—å¹•")
            # å¤åˆ¶åŸè§†é¢‘
            import shutil
            shutil.copy(video_path, output_path)
            return

        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")

        # è·å–è§†é¢‘ä¿¡æ¯
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # åŠ è½½å­—ä½“
        try:
            font = ImageFont.truetype(font_path, font_size)
        except Exception as e:
            _log_error(f"åŠ è½½å­—ä½“å¤±è´¥: {e}")
            font = ImageFont.load_default()

        # é¢œè‰²æ˜ å°„
        color_map = {
            "white": (255, 255, 255),
            "yellow": (255, 255, 0),
            "black": (0, 0, 0),
            "red": (255, 0, 0),
            "green": (0, 255, 0),
            "blue": (0, 0, 255)
        }
        font_rgb = color_map.get(font_color, (255, 255, 0))
        stroke_rgb = color_map.get(stroke_color, (0, 0, 0)) if stroke_width > 0 else None

        # èƒŒæ™¯è®¾ç½®
        bg_opacity = 0.5 if background == "yes" else 0.0
        bg_rgb = (0, 0, 0)  # é»‘è‰²èƒŒæ™¯

        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, self.fps, (self.video_width, self.video_height))

        # ç”ŸæˆåŠ¨æ€å­—å¹•ç‰‡æ®µ
        line_width_ratio = 0.9  # å›ºå®šå€¼
        dynamic_segments = self._generate_dynamic_segments(
            words_list, max_lines, clearance_threshold, line_width_ratio, font, font_size
        )

        _log_info(f"ğŸ“ ç”Ÿæˆäº† {len(dynamic_segments)} ä¸ªåŠ¨æ€å­—å¹•ç‰‡æ®µ")

        # é€å¸§å¤„ç†
        frame_idx = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            current_time = frame_idx / self.fps

            # æŸ¥æ‰¾å½“å‰æ—¶é—´çš„å­—å¹•
            current_text = None

            for start, end, text in dynamic_segments:
                # å»¶é•¿å­—å¹•æ˜¾ç¤ºæ—¶é—´
                extended_end = end + subtitle_extend_time
                if start <= current_time <= extended_end:
                    current_text = text
                    break

            # å¦‚æœæœ‰å­—å¹•ï¼Œç»˜åˆ¶
            if current_text:
                # ç»˜åˆ¶å­—å¹•ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸ä½¿ç”¨åŠ¨ç”»ï¼‰
                frame = self._draw_dynamic_subtitle_on_frame(
                    frame,
                    current_text,
                    font,
                    font_rgb,
                    bg_rgb,
                    bg_opacity,
                    text_direction,
                    position,
                    stroke_width,
                    stroke_rgb,
                    offset_x,
                    offset_y
                )

            out.write(frame)
            frame_idx += 1

            # è¿›åº¦æ˜¾ç¤º
            if frame_idx % 30 == 0:
                progress = (frame_idx / total_frames) * 100
                _log_info(f"è¿›åº¦: {progress:.1f}% ({frame_idx}/{total_frames})")

        cap.release()
        out.release()

        # ä½¿ç”¨ ffmpeg åˆå¹¶éŸ³é¢‘
        _log_info(f"ğŸµ æ­£åœ¨åˆå¹¶éŸ³é¢‘...")
        self._merge_audio(video_path, output_path)

        _log_info(f"âœ… åŠ¨æ€å­—å¹•æ¸²æŸ“å®Œæˆ: {output_path}")

    def _generate_dynamic_segments(
        self,
        words_list: List[Tuple[float, float, str]],
        max_lines: int,
        clearance_threshold: float,
        line_width_ratio: float,
        font: ImageFont.FreeTypeFont,
        font_size: int
    ) -> List[Tuple[float, float, str]]:
        """
        ç”ŸæˆåŠ¨æ€å­—å¹•ç‰‡æ®µï¼ˆé€è¯ç´¯ç§¯æ˜¾ç¤ºï¼‰

        Returns:
            [(start, end, accumulated_text), ...]
        """
        # æ£€æµ‹æ˜¯å¦æ˜¯é€å¥æ—¶é—´æˆ³ï¼ˆæ¯ä¸ª"è¯"çš„å¹³å‡é•¿åº¦ > 10ï¼‰
        if words_list:
            avg_word_length = sum(len(word) for _, _, word in words_list if word != "<NEWLINE>") / max(1, len([w for _, _, w in words_list if w != "<NEWLINE>"]))
            is_sentence_timestamps = avg_word_length > 10

            if is_sentence_timestamps:
                _log_info(f"ğŸ” æ£€æµ‹åˆ°é€å¥æ—¶é—´æˆ³ï¼ˆå¹³å‡é•¿åº¦: {avg_word_length:.1f}ï¼‰ï¼Œæ¯ä¸ªå¥å­ç‹¬ç«‹æ˜¾ç¤ºå¹¶æ™ºèƒ½æ¢è¡Œ")
        else:
            is_sentence_timestamps = False

        segments = []

        # å¦‚æœæ˜¯é€å¥æ—¶é—´æˆ³ï¼Œé€å¥ç´¯ç§¯æ˜¾ç¤º
        if is_sentence_timestamps:
            import string
            # ä¸­æ–‡å’Œè‹±æ–‡æ ‡ç‚¹ç¬¦å·
            punctuation = string.punctuation + 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹ã€Â·â€¦â€”'

            accumulated_text = ""
            for idx, (start, end, sentence) in enumerate(words_list):
                if sentence == "<NEWLINE>":
                    continue

                # å»é™¤æ ‡ç‚¹ç¬¦å·
                sentence_no_punct = ''.join([c for c in sentence if c not in punctuation])

                if not sentence_no_punct.strip():
                    continue

                # å¯¹å¥å­è¿›è¡Œæ™ºèƒ½æ¢è¡Œ
                max_width = int(self.video_width * 0.9)
                wrapped_sentence = smart_wrap_text(sentence_no_punct, max_width, font.path, font_size, 'zh')

                # ç´¯ç§¯å¥å­
                if accumulated_text:
                    accumulated_text += "\n" + wrapped_sentence
                else:
                    accumulated_text = wrapped_sentence

                segments.append((start, end, accumulated_text))
        else:
            # é€è¯æ—¶é—´æˆ³ï¼šç´¯ç§¯æ˜¾ç¤º
            import string
            # ä¸­æ–‡å’Œè‹±æ–‡æ ‡ç‚¹ç¬¦å·
            punctuation = string.punctuation + 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹ã€Â·â€¦â€”'

            current_text = ""
            segment_start = 0.0
            last_word_end = 0.0

            for i, (start, end, word) in enumerate(words_list):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ¢è¡Œç¬¦æ ‡è®°ï¼ˆä½¿ç”¨ <NEWLINE> æ ‡è®°ï¼‰
                if word == "<NEWLINE>":
                    # åœ¨å½“å‰æ–‡æœ¬ä¸­æ·»åŠ æ¢è¡Œç¬¦ï¼ˆä¿æŒç´¯ç§¯ï¼Œä¸æ¸…ç©ºï¼‰
                    if current_text:
                        current_text += "\n"
                        segments.append((segment_start, end, current_text))
                    last_word_end = end
                    continue

                # å»é™¤æ ‡ç‚¹ç¬¦å·
                word_no_punct = ''.join([c for c in word if c not in punctuation])

                # å¦‚æœå»é™¤æ ‡ç‚¹åä¸ºç©ºï¼Œè·³è¿‡
                if not word_no_punct.strip():
                    last_word_end = end
                    continue

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç©ºï¼ˆé™éŸ³è¶…è¿‡é˜ˆå€¼ï¼‰
                if current_text and (start - last_word_end) > clearance_threshold:
                    # ä¿å­˜å½“å‰ç‰‡æ®µ
                    segments.append((segment_start, last_word_end, current_text))
                    current_text = ""
                    segment_start = start

                # ç´¯ç§¯æ–‡å­—
                if not current_text:
                    current_text = word_no_punct
                    segment_start = start
                else:
                    current_text += word_no_punct

                # ä¿å­˜å½“å‰ç‰‡æ®µï¼ˆæ¯ä¸ªè¯éƒ½åˆ›å»ºä¸€ä¸ªç‰‡æ®µï¼‰
                segments.append((segment_start, end, current_text))

                last_word_end = end

        return segments

    def _draw_dynamic_subtitle_on_frame(
        self,
        frame: np.ndarray,
        text: str,
        font: ImageFont.FreeTypeFont,
        font_color: Tuple[int, int, int],
        bg_color: Tuple[int, int, int],
        bg_opacity: float,
        text_direction: str,
        position: str,
        stroke_width: float,
        stroke_color: Optional[Tuple[int, int, int]],
        offset_x: int = 0,
        offset_y: int = 0
    ) -> np.ndarray:
        """åœ¨å¸§ä¸Šç»˜åˆ¶åŠ¨æ€å­—å¹•ï¼ˆç®€åŒ–ç‰ˆï¼‰"""

        # è½¬æ¢ä¸º PIL Image (RGB)
        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_image, 'RGBA')

        # è®¡ç®—æ–‡å­—å°ºå¯¸å’Œä½ç½®
        if text_direction == "vertical":
            text_width, text_height = self._get_vertical_text_size(draw, text, font)
        else:
            # åŠ¨æ€å­—å¹•ï¼šæ–‡æœ¬å·²ç»åœ¨ _generate_dynamic_segments ä¸­è¿›è¡Œäº†æ™ºèƒ½æ¢è¡Œ
            bbox = draw.multiline_textbbox((0, 0), text, font=font, spacing=3)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]

        # è®¡ç®—ä½ç½®
        # ä½ç½®æ ¼å¼ï¼šå³ä¸Šã€å³ä¸­ã€å³ä¸‹ã€ä¸­ä¸Šã€æ­£ä¸­ã€ä¸­ä¸‹ã€å·¦ä¸Šã€å·¦ä¸­ã€å·¦ä¸‹
        # å…¼å®¹æ—§æ ¼å¼ï¼šbottom -> ä¸­ä¸‹, top -> ä¸­ä¸Š, middle -> æ­£ä¸­
        position_map = {
            "bottom": "ä¸­ä¸‹",
            "top": "ä¸­ä¸Š",
            "middle": "æ­£ä¸­",
            "ä¸­ä¸­": "æ­£ä¸­"  # å…¼å®¹æ—§çš„"ä¸­ä¸­"
        }
        position = position_map.get(position, position)

        # è§£æä½ç½®
        if position in ["å³ä¸Š", "å³ä¸­", "å³ä¸‹"]:
            # å³ä¾§
            horizontal_offset = int(self.video_width * 0.05)
            x = self.video_width - text_width - horizontal_offset
        elif position in ["ä¸­ä¸Š", "æ­£ä¸­", "ä¸­ä¸‹"]:
            # ä¸­é—´
            x = (self.video_width - text_width) // 2
        else:  # å·¦ä¸Šã€å·¦ä¸­ã€å·¦ä¸‹
            # å·¦ä¾§
            horizontal_offset = int(self.video_width * 0.05)
            x = horizontal_offset

        if position in ["å³ä¸Š", "ä¸­ä¸Š", "å·¦ä¸Š"]:
            # ä¸Šæ–¹
            vertical_offset = int(self.video_height * 0.05)
            y = vertical_offset
        elif position in ["å³ä¸­", "æ­£ä¸­", "å·¦ä¸­"]:
            # ä¸­é—´
            y = (self.video_height - text_height) // 2
        else:  # å³ä¸‹ã€ä¸­ä¸‹ã€å·¦ä¸‹
            # ä¸‹æ–¹
            vertical_offset = int(self.video_height * 0.05)
            y = self.video_height - text_height - vertical_offset

        # åº”ç”¨åç§»é‡
        # offset_x: æ­£æ•°å‘å³ï¼Œè´Ÿæ•°å‘å·¦
        # offset_y: æ­£æ•°å‘ä¸Šï¼Œè´Ÿæ•°å‘ä¸‹ï¼ˆæ³¨æ„ï¼šå›¾åƒåæ ‡ç³» Y è½´å‘ä¸‹ï¼Œæ‰€ä»¥è¦å–åï¼‰
        x += offset_x
        y -= offset_y

        # ç»˜åˆ¶èƒŒæ™¯
        if bg_opacity > 0:
            bg_padding = 10
            bg_x1 = x - bg_padding
            bg_y1 = y - bg_padding
            bg_x2 = x + text_width + bg_padding
            bg_y2 = y + text_height + bg_padding
            draw.rectangle(
                [bg_x1, bg_y1, bg_x2, bg_y2],
                fill=(*bg_color, int(255 * bg_opacity))
            )

        # ç»˜åˆ¶æ–‡å­—
        if text_direction == "vertical":
            self._draw_vertical_text(draw, text, font, x, y, font_color, 1.0, stroke_width, stroke_color)
        else:
            # æè¾¹æ•ˆæœ
            if stroke_width > 0 and stroke_color:
                for stroke_offset_x in range(-int(stroke_width), int(stroke_width) + 1):
                    for stroke_offset_y in range(-int(stroke_width), int(stroke_width) + 1):
                        if stroke_offset_x != 0 or stroke_offset_y != 0:
                            draw.multiline_text(
                                (x + stroke_offset_x, y + stroke_offset_y),
                                text,
                                font=font,
                                fill=(*stroke_color, 255),
                                spacing=10,
                                align='center'
                            )

            # ä¸»æ–‡å­—
            draw.multiline_text(
                (x, y),
                text,
                font=font,
                fill=(*font_color, 255),
                spacing=10,
                align='center'
            )

        # è½¬æ¢å› OpenCV æ ¼å¼ (BGR)
        return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

    def _merge_audio(self, source_video: str, target_video: str):
        """
        ä½¿ç”¨ ffmpeg å°†æºè§†é¢‘çš„éŸ³é¢‘åˆå¹¶åˆ°ç›®æ ‡è§†é¢‘

        Args:
            source_video: æºè§†é¢‘è·¯å¾„ï¼ˆåŒ…å«éŸ³é¢‘ï¼‰
            target_video: ç›®æ ‡è§†é¢‘è·¯å¾„ï¼ˆåªæœ‰è§†é¢‘æµï¼Œéœ€è¦æ·»åŠ éŸ³é¢‘ï¼‰
        """
        import subprocess
        import tempfile

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_output = target_video + ".temp.mp4"

        try:
            # ä½¿ç”¨ ffmpeg åˆå¹¶éŸ³é¢‘
            # -i target_video: è¾“å…¥ç›®æ ‡è§†é¢‘ï¼ˆåªæœ‰è§†é¢‘æµï¼‰
            # -i source_video: è¾“å…¥æºè§†é¢‘ï¼ˆåŒ…å«éŸ³é¢‘ï¼‰
            # -map 0:v:0: ä½¿ç”¨ç¬¬ä¸€ä¸ªè¾“å…¥çš„è§†é¢‘æµ
            # -map 1:a:0?: ä½¿ç”¨ç¬¬äºŒä¸ªè¾“å…¥çš„éŸ³é¢‘æµï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            # -c:v copy: è§†é¢‘æµç›´æ¥å¤åˆ¶ï¼Œä¸é‡æ–°ç¼–ç 
            # -c:a aac: éŸ³é¢‘æµä½¿ç”¨ AAC ç¼–ç 
            # -shortest: ä»¥æœ€çŸ­çš„æµä¸ºå‡†
            cmd = [
                'ffmpeg',
                '-i', target_video,
                '-i', source_video,
                '-map', '0:v:0',
                '-map', '1:a:0?',
                '-c:v', 'copy',
                '-c:a', 'aac',
                '-shortest',
                '-y',  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                temp_output
            ]

            # æ‰§è¡Œ ffmpeg å‘½ä»¤
            result = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )

            if result.returncode == 0:
                # æˆåŠŸï¼šæ›¿æ¢åŸæ–‡ä»¶
                import shutil
                shutil.move(temp_output, target_video)
                _log_info(f"âœ… éŸ³é¢‘åˆå¹¶æˆåŠŸ")
            else:
                # å¤±è´¥ï¼šä¿ç•™åŸæ–‡ä»¶ï¼ˆæ— éŸ³é¢‘ï¼‰
                _log_error(f"âŒ éŸ³é¢‘åˆå¹¶å¤±è´¥: {result.stderr}")
                if os.path.exists(temp_output):
                    os.remove(temp_output)
                _log_info(f"âš ï¸ ä¿ç•™æ— éŸ³é¢‘çš„è§†é¢‘æ–‡ä»¶")

        except FileNotFoundError:
            _log_error(f"âŒ æœªæ‰¾åˆ° ffmpegï¼Œæ— æ³•åˆå¹¶éŸ³é¢‘")
            _log_error(f"âš ï¸ è¯·å®‰è£… ffmpeg: https://ffmpeg.org/download.html")
            if os.path.exists(temp_output):
                os.remove(temp_output)

        except Exception as e:
            _log_error(f"âŒ éŸ³é¢‘åˆå¹¶å‡ºé”™: {str(e)}")
            if os.path.exists(temp_output):
                os.remove(temp_output)

