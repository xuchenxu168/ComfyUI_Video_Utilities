"""
ASR Engine Wrapper
æ”¯æŒ faster-whisper å¼•æ“ï¼Œä½¿ç”¨ ComfyUI/models/TTS ç›®å½•ä¸‹çš„æ¨¡å‹
"""

import os
import re
import torch
import tempfile
from typing import List, Tuple, Optional

# æ—¥å¿—å‡½æ•°
def _log_info(message):
    print(f"[ASR] {message}")

def _log_error(message):
    print(f"[ASR ERROR] {message}")

def _log_warning(message):
    print(f"[ASR WARNING] âš ï¸ {message}")


def _download_model(model_name: str, model_path: str) -> bool:
    """
    è‡ªåŠ¨ä¸‹è½½æ¨¡å‹

    Args:
        model_name: æ¨¡å‹åç§°
        model_path: æ¨¡å‹ä¿å­˜è·¯å¾„

    Returns:
        æ˜¯å¦ä¸‹è½½æˆåŠŸ
    """
    # æ¨¡å‹ä»“åº“æ˜ å°„
    MODEL_REPOS = {
        # CTranslate2 æ ¼å¼ï¼ˆæ¨èï¼‰
        "Belle-whisper-large-v3-zh-punct-ct2": "k1nto/Belle-whisper-large-v3-zh-punct-ct2",
        "Belle-whisper-large-v3-zh-punct-ct2-float32": "k1nto/Belle-whisper-large-v3-zh-punct-ct2-float32",
        "whisper-large-v3-ct2": "Systran/faster-whisper-large-v3",
        "whisper-medium-ct2": "Systran/faster-whisper-medium",
        "whisper-small-ct2": "Systran/faster-whisper-small",

        # Transformers æ ¼å¼ï¼ˆéœ€è¦ torchcodecï¼‰
        "Belle-whisper-large-v3-zh-punct": "BELLE-2/Belle-whisper-large-v3-zh-punct",
        "Belle-whisper-large-v3-zh": "BELLE-2/Belle-whisper-large-v3-zh",
    }

    if model_name not in MODEL_REPOS:
        _log_warning(f"æœªçŸ¥æ¨¡å‹: {model_name}ï¼Œæ— æ³•è‡ªåŠ¨ä¸‹è½½")
        _log_warning(f"æ”¯æŒçš„æ¨¡å‹: {', '.join(MODEL_REPOS.keys())}")
        return False

    repo_id = MODEL_REPOS[model_name]

    try:
        _log_info(f"ğŸ“¥ å¼€å§‹è‡ªåŠ¨ä¸‹è½½æ¨¡å‹: {model_name}")
        _log_info(f"ğŸ“¦ ä»“åº“: {repo_id}")
        _log_info(f"ğŸ“ ä¿å­˜è·¯å¾„: {model_path}")
        _log_info(f"â³ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ ååˆ†é’Ÿï¼Œå–å†³äºç½‘é€Ÿ...")

        # å°è¯•å¯¼å…¥ huggingface_hub
        try:
            from huggingface_hub import snapshot_download
        except ImportError:
            _log_warning("huggingface_hub æœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
            import subprocess
            import sys
            subprocess.check_call([sys.executable, "-m", "pip", "install", "huggingface-hub"])
            from huggingface_hub import snapshot_download

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é•œåƒç«™ç‚¹
        try:
            import sys
            sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
            from config import USE_HF_MIRROR, HF_MIRROR_ENDPOINT, DOWNLOAD_TIMEOUT

            if USE_HF_MIRROR:
                _log_info(f"ğŸŒ ä½¿ç”¨é•œåƒç«™ç‚¹: {HF_MIRROR_ENDPOINT}")
                os.environ["HF_ENDPOINT"] = HF_MIRROR_ENDPOINT
        except ImportError:
            # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
            DOWNLOAD_TIMEOUT = 3600

        # ä¸‹è½½æ¨¡å‹
        snapshot_download(
            repo_id=repo_id,
            local_dir=model_path,
            local_dir_use_symlinks=False,
            resume_download=True
        )

        _log_info(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ: {model_name}")
        return True

    except Exception as e:
        _log_warning(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {str(e)}")
        _log_warning(f"è¯·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹:")
        _log_warning(f"  pip install huggingface-hub")
        _log_warning(f"  huggingface-cli download {repo_id} --local-dir {model_path}")
        return False


class ASREngine:
    """ASR å¼•æ“å°è£…ç±»"""

    def __init__(self):
        self.model_cache = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.current_model_name = None

        # è·å– ComfyUI models ç›®å½•
        try:
            import folder_paths
            models_dir = folder_paths.models_dir
            self.model_path = os.path.join(models_dir, "TTS")
            _log_info(f"ğŸ“ æ¨¡å‹ç›®å½•: {self.model_path}")
        except Exception as e:
            _log_error(f"æ— æ³•è·å– models ç›®å½•: {e}")
            self.model_path = None
    
    def recognize(
        self,
        audio_path: str,
        engine: str,
        model_name: str,
        language: str,
        prompt: Optional[str] = None,
        max_sentence_length: int = 20
    ) -> Tuple[List[Tuple[float, float, str]], List[Tuple[float, float, str]]]:
        """
        è¯­éŸ³è¯†åˆ«

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            engine: ASR å¼•æ“ï¼ˆå›ºå®šä¸º faster-whisperï¼‰
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ Belle-whisper-large-v3-zh-punct-ct2ï¼‰
            language: è¯­è¨€ä»£ç 
            prompt: æç¤ºæ–‡æœ¬
            max_sentence_length: æœ€å¤§å¥å­é•¿åº¦

        Returns:
            (words_list, sentences_list)
            words_list: [(start, end, word), ...]
            sentences_list: [(start, end, sentence), ...]
        """
        _log_info(f"ğŸ¤ ASR å¼•æ“: {engine}, æ¨¡å‹: {model_name}, è¯­è¨€: {language}")

        # åˆ¤æ–­æ¨¡å‹æ ¼å¼ï¼šCTranslate2 æˆ– Transformers
        # CTranslate2 æ ¼å¼çš„æ¨¡å‹åç§°åŒ…å« "-ct2"ï¼ˆå¯èƒ½åé¢è¿˜æœ‰å…¶ä»–åç¼€ï¼Œå¦‚ -float32ï¼‰
        if "-ct2" in model_name:
            # CTranslate2 æ ¼å¼ï¼ˆfaster-whisperï¼‰
            return self._recognize_faster_whisper(audio_path, model_name, language, prompt, max_sentence_length)
        else:
            # Transformers æ ¼å¼ï¼ˆåŸç”Ÿ Whisperï¼‰
            return self._recognize_transformers_whisper(audio_path, model_name, language, prompt, max_sentence_length)

    def _recognize_faster_whisper(
        self,
        audio_path: str,
        model_name: str,
        language: str,
        prompt: Optional[str],
        max_sentence_length: int
    ) -> Tuple[List[Tuple[float, float, str]], List[Tuple[float, float, str]]]:
        """faster-whisper è¯†åˆ«"""
        try:
            from faster_whisper import WhisperModel
        except ImportError:
            raise ImportError("è¯·å®‰è£… faster-whisper: pip install faster-whisper")

        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not self.model_path:
            raise ValueError("æ— æ³•è·å–æ¨¡å‹ç›®å½•")

        model_full_path = os.path.join(self.model_path, model_name)

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™å°è¯•è‡ªåŠ¨ä¸‹è½½
        if not os.path.exists(model_full_path):
            _log_warning(f"æ¨¡å‹ä¸å­˜åœ¨: {model_full_path}")

            # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨ä¸‹è½½
            auto_download = True
            try:
                import sys
                sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
                from config import AUTO_DOWNLOAD_MODELS
                auto_download = AUTO_DOWNLOAD_MODELS
            except ImportError:
                pass  # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé»˜è®¤å¯ç”¨è‡ªåŠ¨ä¸‹è½½

            if auto_download:
                _log_info(f"ğŸš€ å°è¯•è‡ªåŠ¨ä¸‹è½½æ¨¡å‹...")

                # å°è¯•è‡ªåŠ¨ä¸‹è½½
                if _download_model(model_name, model_full_path):
                    _log_info(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸï¼Œç»§ç»­åŠ è½½...")
                else:
                    # ä¸‹è½½å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
                    raise FileNotFoundError(
                        f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ä¸”è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {model_full_path}\n"
                        f"è¯·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å¹¶æ”¾ç½®åˆ° ComfyUI/models/TTS/{model_name} ç›®å½•\n"
                        f"æ¨èæ¨¡å‹:\n"
                        f"  - Belle-whisper-large-v3-zh-punct-ct2 (ä¸­æ–‡)\n"
                        f"  - Belle-whisper-large-v3-zh-punct-ct2-float32 (ä¸­æ–‡, float32)\n"
                        f"  - whisper-large-v3-ct2 (å¤šè¯­è¨€)\n"
                        f"ä¸‹è½½å‘½ä»¤:\n"
                        f"  pip install huggingface-hub\n"
                        f"  huggingface-cli download <repo_id> --local-dir {model_full_path}"
                    )
            else:
                # è‡ªåŠ¨ä¸‹è½½å·²ç¦ç”¨
                raise FileNotFoundError(
                    f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_full_path}\n"
                    f"è‡ªåŠ¨ä¸‹è½½å·²ç¦ç”¨ï¼ˆåœ¨ config.py ä¸­è®¾ç½® AUTO_DOWNLOAD_MODELS = True å¯ç”¨ï¼‰\n"
                    f"è¯·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å¹¶æ”¾ç½®åˆ° ComfyUI/models/TTS/{model_name} ç›®å½•\n"
                    f"æ¨èæ¨¡å‹:\n"
                    f"  - Belle-whisper-large-v3-zh-punct-ct2 (ä¸­æ–‡)\n"
                    f"  - Belle-whisper-large-v3-zh-punct-ct2-float32 (ä¸­æ–‡, float32)\n"
                    f"  - whisper-large-v3-ct2 (å¤šè¯­è¨€)\n"
                    f"ä¸‹è½½å‘½ä»¤:\n"
                    f"  pip install huggingface-hub\n"
                    f"  huggingface-cli download <repo_id> --local-dir {model_full_path}"
                )

        # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        if self.current_model_name != model_name:
            _log_info(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_name}")
            _log_info(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_full_path}")

            # æ¸…ç©ºä¹‹å‰çš„æ¨¡å‹
            self.model_cache.clear()

            # åŠ è½½æ–°æ¨¡å‹
            self.model_cache["current"] = WhisperModel(
                model_full_path,
                device=self.device,
                compute_type="float16" if self.device == "cuda" else "int8"
            )
            self.current_model_name = model_name
            _log_info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        else:
            _log_info(f"â™»ï¸ ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹: {model_name}")

        model = self.model_cache["current"]

        # faster-whisper è¯†åˆ«ï¼ˆæ”¯æŒ word_timestamps å’Œ promptï¼‰
        _log_info("ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ«...")

        # å‡†å¤‡ transcribe å‚æ•°
        transcribe_kwargs = {
            "language": None if language == "auto" else language,
            "word_timestamps": True
        }

        # å¦‚æœæä¾›äº† promptï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­ï¼ˆç”¨äºæé«˜å‡†ç¡®ç‡ï¼‰
        if prompt and prompt.strip():
            transcribe_kwargs["initial_prompt"] = prompt.strip()
            _log_info(f"ğŸ’¡ ä½¿ç”¨æç¤ºè¯æé«˜å‡†ç¡®ç‡: {prompt.strip()[:50]}...")

        segments, info = model.transcribe(audio_path, **transcribe_kwargs)

        _log_info(f"ğŸŒ æ£€æµ‹åˆ°è¯­è¨€: {info.language}")

        # æå–é€è¯å’Œé€å¥æ—¶é—´æˆ³
        words_list = []
        sentences_list = []

        # å°† segments è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿è°ƒè¯•
        segments_list = list(segments)
        _log_info(f"ğŸ” æ¨¡å‹è¿”å›äº† {len(segments_list)} ä¸ª segment")

        for i, segment in enumerate(segments_list):
            # å¥å­çº§åˆ«
            start = round(segment.start, 2)
            end = round(segment.end, 2)
            text = segment.text.strip()
            sentences_list.append((start, end, text))

            # è°ƒè¯•ï¼šæ˜¾ç¤ºå‰ 3 ä¸ª segment çš„ä¿¡æ¯
            if i < 3:
                _log_info(f"  Segment {i+1}: [{start:.2f}s - {end:.2f}s] {text[:50]}...")

            # è¯çº§åˆ«
            if hasattr(segment, 'words') and segment.words:
                for word in segment.words:
                    word_start = round(word.start, 2)
                    word_end = round(word.end, 2)
                    word_text = word.word.strip()
                    words_list.append((word_start, word_end, word_text))

        _log_info(f"ğŸ“ è¯†åˆ«äº† {len(sentences_list)} ä¸ªå¥å­, {len(words_list)} ä¸ªè¯")

        # å¦‚æœåªæœ‰1ä¸ªå¥å­ä¸”æ–‡æœ¬å¾ˆé•¿ï¼Œå¼ºåˆ¶åˆ†å¥
        if len(sentences_list) == 1 and len(sentences_list[0][2]) > 30:
            _log_info(f"ğŸ”§ æ£€æµ‹åˆ°åªæœ‰1ä¸ªé•¿å¥å­ï¼Œä½¿ç”¨æ ‡ç‚¹ç¬¦å·å¼ºåˆ¶åˆ†å¥...")
            sentences_list = self._force_split_sentences(sentences_list)
            _log_info(f"âœ… å¼ºåˆ¶åˆ†å¥åå¾—åˆ° {len(sentences_list)} ä¸ªå¥å­")

        # å¦‚æœæ¨¡å‹ä¸æ”¯æŒé€è¯æ—¶é—´æˆ³ï¼Œä½¿ç”¨ jieba åˆ†è¯ç”Ÿæˆä¼ªæ—¶é—´æˆ³
        if not words_list and sentences_list:
            _log_info(f"ğŸ”§ æ¨¡å‹ä¸æ”¯æŒé€è¯æ—¶é—´æˆ³ï¼Œä½¿ç”¨ jieba åˆ†è¯ç”Ÿæˆä¼ªæ—¶é—´æˆ³...")
            words_list = self._generate_words_from_sentences(sentences_list)
            _log_info(f"âœ… ç”Ÿæˆäº† {len(words_list)} ä¸ªè¯çš„ä¼ªæ—¶é—´æˆ³")

        # å¦‚æœéœ€è¦ï¼ŒæŒ‰æœ€å¤§é•¿åº¦é‡æ–°åˆ†å¥
        if max_sentence_length > 0:
            sentences_list = self._split_sentences(words_list, sentences_list, max_sentence_length, info.language)

        return words_list, sentences_list

    def _recognize_transformers_whisper(
        self,
        audio_path: str,
        model_name: str,
        language: str,
        prompt: Optional[str],
        max_sentence_length: int
    ) -> Tuple[List[Tuple[float, float, str]], List[Tuple[float, float, str]]]:
        """Transformers Whisper è¯†åˆ«ï¼ˆç”¨äº Belle-whisper-large-v3-zh-punct ç­‰æ¨¡å‹ï¼‰"""
        try:
            from transformers import pipeline
        except ImportError:
            raise ImportError("è¯·å®‰è£… transformers: pip install transformers")

        # æ„å»ºæ¨¡å‹è·¯å¾„
        if not self.model_path:
            raise ValueError("æ— æ³•è·å–æ¨¡å‹ç›®å½•")

        model_full_path = os.path.join(self.model_path, model_name)

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™å°è¯•è‡ªåŠ¨ä¸‹è½½
        if not os.path.exists(model_full_path):
            _log_warning(f"æ¨¡å‹ä¸å­˜åœ¨: {model_full_path}")

            # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨ä¸‹è½½
            auto_download = True
            try:
                import sys
                sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
                from config import AUTO_DOWNLOAD_MODELS
                auto_download = AUTO_DOWNLOAD_MODELS
            except ImportError:
                pass  # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé»˜è®¤å¯ç”¨è‡ªåŠ¨ä¸‹è½½

            if auto_download:
                _log_info(f"ğŸš€ å°è¯•è‡ªåŠ¨ä¸‹è½½æ¨¡å‹...")

                # å°è¯•è‡ªåŠ¨ä¸‹è½½
                if _download_model(model_name, model_full_path):
                    _log_info(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸï¼Œç»§ç»­åŠ è½½...")
                else:
                    # ä¸‹è½½å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
                    raise FileNotFoundError(
                        f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ä¸”è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {model_full_path}\n"
                        f"è¯·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å¹¶æ”¾ç½®åˆ° ComfyUI/models/TTS/{model_name} ç›®å½•\n"
                        f"ä¸‹è½½å‘½ä»¤:\n"
                        f"  pip install huggingface-hub\n"
                        f"  huggingface-cli download BELLE-2/{model_name} --local-dir {model_full_path}"
                    )
            else:
                # è‡ªåŠ¨ä¸‹è½½å·²ç¦ç”¨
                raise FileNotFoundError(
                    f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_full_path}\n"
                    f"è‡ªåŠ¨ä¸‹è½½å·²ç¦ç”¨ï¼ˆåœ¨ config.py ä¸­è®¾ç½® AUTO_DOWNLOAD_MODELS = True å¯ç”¨ï¼‰\n"
                    f"è¯·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å¹¶æ”¾ç½®åˆ° ComfyUI/models/TTS/{model_name} ç›®å½•\n"
                    f"ä¸‹è½½å‘½ä»¤:\n"
                    f"  pip install huggingface-hub\n"
                    f"  huggingface-cli download BELLE-2/{model_name} --local-dir {model_full_path}"
                )

        # åŠ è½½æˆ–ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹
        if self.current_model_name != model_name:
            _log_info(f"ğŸ“¥ åŠ è½½ Transformers Whisper æ¨¡å‹: {model_name}...")
            self.model_cache.clear()

            # åˆ›å»º pipeline
            self.model_cache["current"] = pipeline(
                "automatic-speech-recognition",
                model=model_full_path,
                device=0 if self.device == "cuda" else -1
            )

            # è®¾ç½®å¼ºåˆ¶è§£ç å™¨ IDï¼ˆè¯­è¨€å’Œä»»åŠ¡ï¼‰
            self.model_cache["current"].model.config.forced_decoder_ids = (
                self.model_cache["current"].tokenizer.get_decoder_prompt_ids(
                    language="zh" if language == "auto" else language,
                    task="transcribe"
                )
            )

            self.current_model_name = model_name
            _log_info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

        transcriber = self.model_cache["current"]

        # æ‰§è¡Œè¯†åˆ«ï¼ˆTransformers pipeline è¿”å›å¥å­çº§åˆ«çš„ç»“æœï¼‰
        _log_info(f"ğŸ¤ å¼€å§‹è¯†åˆ«...")

        # å‡†å¤‡è¯†åˆ«å‚æ•°
        transcribe_kwargs = {
            "return_timestamps": True,  # è¿”å›æ—¶é—´æˆ³
            "chunk_length_s": 30,  # 30ç§’åˆ†å—
            "stride_length_s": 5   # 5ç§’é‡å 
        }

        # å¦‚æœæä¾›äº† promptï¼Œæ·»åŠ åˆ° generate_kwargs ä¸­ï¼ˆç”¨äºæé«˜å‡†ç¡®ç‡ï¼‰
        if prompt and prompt.strip():
            transcribe_kwargs["generate_kwargs"] = {"prompt_ids": transcriber.tokenizer.encode(prompt.strip())}
            _log_info(f"ğŸ’¡ ä½¿ç”¨æç¤ºè¯æé«˜å‡†ç¡®ç‡: {prompt.strip()[:50]}...")

        result = transcriber(audio_path, **transcribe_kwargs)

        # æå–å¥å­çº§åˆ«çš„æ—¶é—´æˆ³
        sentences_list = []
        words_list = []

        if "chunks" in result:
            # æœ‰æ—¶é—´æˆ³ä¿¡æ¯
            _log_info(f"ğŸ” Transformers è¿”å›äº† {len(result['chunks'])} ä¸ª chunk")
            for i, chunk in enumerate(result["chunks"]):
                start = round(chunk["timestamp"][0], 2) if chunk["timestamp"][0] is not None else 0.0
                end = round(chunk["timestamp"][1], 2) if chunk["timestamp"][1] is not None else 0.0
                text = chunk["text"].strip()
                if text:
                    sentences_list.append((start, end, text))
                    # è°ƒè¯•ï¼šæ˜¾ç¤ºå‰ 3 ä¸ª chunk çš„ä¿¡æ¯
                    if i < 3:
                        _log_info(f"  Chunk {i+1}: [{start:.2f}s - {end:.2f}s] {text[:50]}...")
        else:
            # æ²¡æœ‰æ—¶é—´æˆ³ä¿¡æ¯ï¼Œä½¿ç”¨æ•´ä¸ªæ–‡æœ¬
            _log_warning(f"âš ï¸ Transformers æ²¡æœ‰è¿”å› chunksï¼Œæ‰€æœ‰æ–‡æœ¬å°†ä½œä¸ºä¸€ä¸ªå¥å­")
            text = result["text"].strip()
            if text:
                sentences_list.append((0.0, 0.0, text))

        _log_info(f"ğŸ“ è¯†åˆ«äº† {len(sentences_list)} ä¸ªå¥å­")

        # å¦‚æœåªæœ‰1ä¸ªå¥å­ä¸”æ–‡æœ¬å¾ˆé•¿ï¼Œå¼ºåˆ¶åˆ†å¥
        if len(sentences_list) == 1 and len(sentences_list[0][2]) > 30:
            _log_info(f"ğŸ”§ æ£€æµ‹åˆ°åªæœ‰1ä¸ªé•¿å¥å­ï¼Œä½¿ç”¨æ ‡ç‚¹ç¬¦å·å¼ºåˆ¶åˆ†å¥...")
            sentences_list = self._force_split_sentences(sentences_list)
            _log_info(f"âœ… å¼ºåˆ¶åˆ†å¥åå¾—åˆ° {len(sentences_list)} ä¸ªå¥å­")

        # Transformers pipeline ä¸æ”¯æŒé€è¯æ—¶é—´æˆ³
        # ä»å¥å­ä¸­ç”Ÿæˆä¼ªé€è¯æ—¶é—´æˆ³ï¼ˆä½¿ç”¨ jieba åˆ†è¯ï¼‰
        if not words_list and sentences_list:
            _log_info(f"ğŸ”§ Transformers æ¨¡å‹ä¸æ”¯æŒé€è¯æ—¶é—´æˆ³ï¼Œä½¿ç”¨ jieba åˆ†è¯ç”Ÿæˆä¼ªæ—¶é—´æˆ³...")
            words_list = self._generate_words_from_sentences(sentences_list)
            _log_info(f"âœ… ç”Ÿæˆäº† {len(words_list)} ä¸ªè¯çš„ä¼ªæ—¶é—´æˆ³")

        return words_list, sentences_list

    def _force_split_sentences(
        self,
        sentences_list: List[Tuple[float, float, str]]
    ) -> List[Tuple[float, float, str]]:
        """
        å¼ºåˆ¶åˆ†å¥ï¼ˆä½¿ç”¨æ ‡ç‚¹ç¬¦å·ï¼‰

        å½“æ¨¡å‹åªè¿”å›1ä¸ªé•¿å¥å­æ—¶ï¼Œä½¿ç”¨æ ‡ç‚¹ç¬¦å·å¼ºåˆ¶åˆ†å¥

        Args:
            sentences_list: å¥å­åˆ—è¡¨ [(start, end, text), ...]

        Returns:
            new_sentences_list: åˆ†å¥åçš„å¥å­åˆ—è¡¨
        """
        import re

        if len(sentences_list) != 1:
            return sentences_list

        start, end, text = sentences_list[0]

        # å¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œä¸éœ€è¦åˆ†å¥
        if len(text) < 30:
            return sentences_list

        # ä½¿ç”¨æ ‡ç‚¹ç¬¦å·åˆ†å¥ï¼ˆä¸­æ–‡å’Œè‹±æ–‡æ ‡ç‚¹ï¼‰
        # åŒ¹é…å¥å­ç»“æŸç¬¦å·ï¼šã€‚ï¼ï¼Ÿ.!?
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.!?]+)', text)

        # é‡æ–°ç»„åˆå¥å­ï¼ˆä¿ç•™æ ‡ç‚¹ç¬¦å·ï¼‰
        split_sentences = []
        for i in range(0, len(sentences) - 1, 2):
            if i + 1 < len(sentences):
                sentence = sentences[i] + sentences[i + 1]
            else:
                sentence = sentences[i]

            sentence = sentence.strip()
            if sentence:
                split_sentences.append(sentence)

        # å¦‚æœæœ€åä¸€ä¸ªå…ƒç´ æ²¡æœ‰æ ‡ç‚¹ç¬¦å·ï¼Œä¹Ÿæ·»åŠ è¿›å»
        if len(sentences) % 2 == 1 and sentences[-1].strip():
            split_sentences.append(sentences[-1].strip())

        # å¦‚æœåˆ†å¥å¤±è´¥ï¼ˆåªæœ‰1ä¸ªå¥å­ï¼‰ï¼Œå°è¯•ä½¿ç”¨é€—å·åˆ†å¥
        if len(split_sentences) <= 1:
            sentences = re.split(r'([ï¼Œ,]+)', text)
            split_sentences = []
            for i in range(0, len(sentences) - 1, 2):
                if i + 1 < len(sentences):
                    sentence = sentences[i] + sentences[i + 1]
                else:
                    sentence = sentences[i]

                sentence = sentence.strip()
                if sentence:
                    split_sentences.append(sentence)

            if len(sentences) % 2 == 1 and sentences[-1].strip():
                split_sentences.append(sentences[-1].strip())

        # å¦‚æœè¿˜æ˜¯åªæœ‰1ä¸ªå¥å­ï¼Œè¿”å›åŸå§‹åˆ—è¡¨
        if len(split_sentences) <= 1:
            return sentences_list

        # è®¡ç®—æ¯ä¸ªå¥å­çš„æ—¶é•¿ï¼ˆæ ¹æ®å­—ç¬¦æ•°æŒ‰æ¯”ä¾‹åˆ†é…ï¼‰
        total_duration = end - start
        total_chars = sum(len(s) for s in split_sentences)

        # ç”Ÿæˆæ–°çš„ sentences_list
        new_sentences_list = []
        current_time = start
        for sentence in split_sentences:
            sentence_start = current_time
            # æ ¹æ®å¥å­é•¿åº¦æŒ‰æ¯”ä¾‹åˆ†é…æ—¶é—´
            sentence_duration = (len(sentence) / total_chars) * total_duration
            sentence_end = current_time + sentence_duration
            new_sentences_list.append((round(sentence_start, 2), round(sentence_end, 2), sentence))
            current_time = sentence_end

        return new_sentences_list

    def _generate_words_from_sentences(
        self,
        sentences_list: List[Tuple[float, float, str]]
    ) -> List[Tuple[float, float, str]]:
        """
        ä»å¥å­åˆ—è¡¨ç”Ÿæˆä¼ªé€è¯æ—¶é—´æˆ³ï¼ˆä½¿ç”¨ jieba åˆ†è¯ï¼‰

        Args:
            sentences_list: å¥å­åˆ—è¡¨ [(start, end, text), ...]

        Returns:
            words_list: è¯åˆ—è¡¨ [(start, end, word), ...]
        """
        import jieba

        words_list = []

        for idx, (sentence_start, sentence_end, sentence_text) in enumerate(sentences_list):
            # ä½¿ç”¨ jieba åˆ†è¯ï¼ˆä¿ç•™ç©ºæ ¼ï¼‰
            words = [w for w in jieba.cut(sentence_text) if w]  # åªè¿‡æ»¤ç©ºå­—ç¬¦ä¸²ï¼Œä¿ç•™ç©ºæ ¼

            if not words:
                continue

            # è®¡ç®—æ¯ä¸ªè¯çš„å¹³å‡æ—¶é•¿
            sentence_duration = sentence_end - sentence_start
            word_duration = sentence_duration / len(words)

            # ä¸ºæ¯ä¸ªè¯åˆ†é…æ—¶é—´æˆ³
            current_time = sentence_start
            for word_idx, word in enumerate(words):
                word_start = current_time
                word_end = current_time + word_duration
                words_list.append((round(word_start, 2), round(word_end, 2), word))
                current_time = word_end

            # åœ¨æ¯ä¸ªå¥å­ç»“æŸåæ·»åŠ æ¢è¡Œç¬¦æ ‡è®°ï¼ˆé™¤äº†æœ€åä¸€ä¸ªå¥å­ï¼‰
            if idx < len(sentences_list) - 1:
                # æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„æ¢è¡Œç¬¦æ ‡è®°ï¼Œæ—¶é—´æˆ³ä¸æœ€åä¸€ä¸ªè¯ç›¸åŒ
                # ä½¿ç”¨ <NEWLINE> æ ‡è®°è€Œä¸æ˜¯ \nï¼Œå› ä¸º \n ä¼šè¢« strip() å»æ‰
                words_list.append((round(word_end, 2), round(word_end, 2), "<NEWLINE>"))

        return words_list

    def _split_sentences(
        self,
        words_list: List[Tuple[float, float, str]],
        sentences_list: List[Tuple[float, float, str]],
        max_length: int,
        language: str
    ) -> List[Tuple[float, float, str]]:
        """æŒ‰æœ€å¤§é•¿åº¦é‡æ–°åˆ†å¥ï¼ˆå‚è€ƒ ComfyUI_ASR çš„å®ç°ï¼‰"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„åˆ†å¥é€»è¾‘
        # æš‚æ—¶è¿”å›åŸå§‹å¥å­åˆ—è¡¨
        return sentences_list

    def unload_model(self):
        """å¸è½½å½“å‰æ¨¡å‹"""
        self.model_cache.clear()
        self.current_model_name = None
        torch.cuda.empty_cache()
        _log_info(f"ğŸ—‘ï¸ å·²å¸è½½æ‰€æœ‰æ¨¡å‹")

